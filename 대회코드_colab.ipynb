{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19cnqxZBxSaTQWv3RWggT2nyq9jn3rGxi",
      "authorship_tag": "ABX9TyOlAC16PHoBU+ac0DlwttV1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7mcWJh3Zrzo"
      },
      "outputs": [],
      "source": [
        "#ì›ì†ŒìŠ¤ ì¶œì²˜: https://github.com/greenjhp/shcard_bigcontest2024_llm\n",
        "!apt install libomp-dev\n",
        "!pip install faiss-gpu\n",
        "!pip install streamlit\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ë³´ì•ˆ ë¹„ë°€\n",
        "from google.colab import userdata\n",
        "import os\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "Fobu0VHEZuMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import faiss\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì •\n",
        "data_path = '/content/drive/MyDrive/shinhan_bigcontest/data'\n",
        "module_path = '/content/drive/MyDrive/shinhan_bigcontest/module'\n",
        "\n",
        "# Gemini ì„¤ì •\n",
        "import google.generativeai as genai\n",
        "\n",
        "GOOGLE_API_KEY=os.environ['GOOGLE_API_KEY']\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Gemini ëª¨ë¸ ì„ íƒ\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "\n",
        "# CSV íŒŒì¼ ë¡œë“œ\n",
        "## ìì²´ ì „ì²˜ë¦¬ë¥¼ ê±°ì¹œ ë°ì´í„° íŒŒì¼ í™œìš©\n",
        "csv_file_path = data_path + \"/JEJU_MCT_DATA_modified.csv\"\n",
        "df = pd.read_csv(os.path.join(data_path, csv_file_path))\n",
        "\n",
        "# ìµœì‹ ì—°ì›” ë°ì´í„°ë§Œ ê°€ì ¸ì˜´\n",
        "df = df[df['ê¸°ì¤€ì—°ì›”'] == df['ê¸°ì¤€ì—°ì›”'].max()].reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "# Streamlit App UI\n",
        "\n",
        "st.set_page_config(page_title=\"ğŸŠì°¸ì‹ í•œ ì œì£¼ ë§›ì§‘!\")\n",
        "\n",
        "# Replicate Credentials\n",
        "with st.sidebar:\n",
        "    st.title(\"ğŸŠì°¸ì‹ í•œ! ì œì£¼ ë§›ì§‘\")\n",
        "\n",
        "    st.write(\"\")\n",
        "\n",
        "    st.subheader(\"ì–¸ë“œë ˆ ê°€ì‹ ë””ê°€?\")\n",
        "\n",
        "    # selectbox ë ˆì´ë¸” ê³µë°± ì œê±°\n",
        "    st.markdown(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "        .stSelectbox label {  /* This targets the label element for selectbox */\n",
        "            display: none;  /* Hides the label element */\n",
        "        }\n",
        "        .stSelectbox div[role='combobox'] {\n",
        "            margin-top: -20px; /* Adjusts the margin if needed */\n",
        "        }\n",
        "        </style>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "    time = st.sidebar.selectbox(\"\", [\"ì•„ì¹¨\", \"ì ì‹¬\", \"ì˜¤í›„\", \"ì €ë…\", \"ë°¤\"], key=\"time\")\n",
        "\n",
        "    st.write(\"\")\n",
        "\n",
        "    st.subheader(\"ì–´ë“œë ˆê°€ ë§˜ì— ë“œì‹ ë””ê°€?\")\n",
        "\n",
        "    # radio ë ˆì´ë¸” ê³µë°± ì œê±°\n",
        "    st.markdown(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "        .stRadio > label {\n",
        "            display: none;\n",
        "        }\n",
        "        .stRadio > div {\n",
        "            margin-top: -20px;\n",
        "        }\n",
        "        </style>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "    local_choice = st.radio(\n",
        "        '',\n",
        "        ('ì œì£¼ë„ë¯¼ ë§›ì§‘', 'ê´€ê´‘ê° ë§›ì§‘')\n",
        "    )\n",
        "\n",
        "    st.write(\"\")\n",
        "\n",
        "st.title(\"í˜¼ì € ì˜µì„œì˜ˆ!ğŸ‘‹\")\n",
        "st.subheader(\"êµ°ë§›ë‚œ ì œì£¼ ë°¥ì§‘ğŸ§‘â€ğŸ³ ì¶”ì²œí•´ë“œë¦´ê²Œì˜ˆ\")\n",
        "\n",
        "st.write(\"\")\n",
        "\n",
        "st.write(\"#í‘ë¼ì§€ #ê°ˆì¹˜ì¡°ë¦¼ #ì˜¥ë”êµ¬ì´ #ê³ ì‚¬ë¦¬í•´ì¥êµ­ #ì „ë³µëšë°°ê¸° #í•œì¹˜ë¬¼íšŒ #ë¹™ë–¡ #ì˜¤ë©”ê¸°ë–¡..ğŸ¤¤\")\n",
        "\n",
        "st.write(\"\")\n",
        "\n",
        "image_path = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRTHBMuNn2EZw3PzOHnLjDg_psyp-egZXcclWbiASta57PBiKwzpW5itBNms9VFU8UwEMQ&usqp=CAU\"\n",
        "image_html = f\"\"\"\n",
        "<div style=\"display: flex; justify-content: center;\">\n",
        "    <img src=\"{image_path}\" alt=\"centered image\" width=\"50%\">\n",
        "</div>\n",
        "\"\"\"\n",
        "st.markdown(image_html, unsafe_allow_html=True)\n",
        "\n",
        "st.write(\"\")\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "\n",
        "# RAG\n",
        "\n",
        "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Hugging Faceì˜ ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "model_name = \"jhgan/ko-sroberta-multitask\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "embedding_model = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "print(f'Device is {device}.')\n",
        "\n",
        "\n",
        "# FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜\n",
        "def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index.index')):\n",
        "    \"\"\"\n",
        "    FAISS ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "\n",
        "    Parameters:\n",
        "    index_path (str): ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ.\n",
        "\n",
        "    Returns:\n",
        "    faiss.Index: ë¡œë“œëœ FAISS ì¸ë±ìŠ¤ ê°ì²´.\n",
        "    \"\"\"\n",
        "    if os.path.exists(index_path):\n",
        "        # ì¸ë±ìŠ¤ íŒŒì¼ì—ì„œ ë¡œë“œ\n",
        "        index = faiss.read_index(index_path)\n",
        "        print(f\"FAISS ì¸ë±ìŠ¤ê°€ {index_path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "        return index\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"{index_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "# í…ìŠ¤íŠ¸ ì„ë² ë”©\n",
        "def embed_text(text):\n",
        "    # í† í¬ë‚˜ì´ì €ì˜ ì¶œë ¥ë„ GPUë¡œ ì´ë™\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        # ëª¨ë¸ì˜ ì¶œë ¥ì„ GPUì—ì„œ ì—°ì‚°í•˜ê³ , í•„ìš”í•œ ë¶€ë¶„ì„ ê°€ì ¸ì˜´\n",
        "        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)\n",
        "    return embeddings.squeeze().cpu().numpy()  # ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ê³  numpy ë°°ì—´ë¡œ ë³€í™˜\n",
        "\n",
        "# ì„ë² ë”© ë¡œë“œ\n",
        "embeddings = np.load(os.path.join(module_path, 'embeddings_array_file.npy'))\n",
        "\n",
        "def generate_response_with_faiss(question, df, embeddings, model, embed_text, time, local_choice, index_path=os.path.join(module_path, 'faiss_index.index'), max_count=10, k=3, print_prompt=True):\n",
        "    filtered_df = df\n",
        "\n",
        "    # FAISS ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ\n",
        "    index = load_faiss_index(index_path)\n",
        "\n",
        "    # ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
        "    query_embedding = embed_text(question).reshape(1, -1)\n",
        "\n",
        "    # ê°€ì¥ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (3ë°°ìˆ˜)\n",
        "    distances, indices = index.search(query_embedding, k*3)\n",
        "\n",
        "    # FAISSë¡œ ê²€ìƒ‰ëœ ìƒìœ„ kê°œì˜ ë°ì´í„°í”„ë ˆì„ ì¶”ì¶œ\n",
        "    filtered_df = filtered_df.iloc[indices[0, :]].copy().reset_index(drop=True)\n",
        "\n",
        "\n",
        "    # ì›¹í˜ì´ì§€ì˜ ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒí•˜ëŠ” ì˜ì—…ì‹œê°„, í˜„ì§€ì¸ ë§›ì§‘ ì¡°ê±´ êµ¬í˜„\n",
        "\n",
        "    # ì˜ì—…ì‹œê°„ ì˜µì…˜\n",
        "    # í•„í„°ë§ ì¡°ê±´ìœ¼ë¡œ í™œìš©\n",
        "\n",
        "    # ì˜ì—…ì‹œê°„ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°€ê²Œë“¤ë§Œ í•„í„°ë§\n",
        "    if time == 'ì•„ì¹¨':\n",
        "        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(5, 12)))].reset_index(drop=True)\n",
        "    elif time == 'ì ì‹¬':\n",
        "        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(12, 14)))].reset_index(drop=True)\n",
        "    elif time == 'ì˜¤í›„':\n",
        "        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(14, 18)))].reset_index(drop=True)\n",
        "    elif time == 'ì €ë…':\n",
        "        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(18, 23)))].reset_index(drop=True)\n",
        "    elif time == 'ë°¤':\n",
        "        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in [23, 24, 1, 2, 3, 4]))].reset_index(drop=True)\n",
        "\n",
        "    # í•„í„°ë§ í›„ ê°€ê²Œê°€ ì—†ìœ¼ë©´ ë©”ì‹œì§€ë¥¼ ë°˜í™˜\n",
        "    if filtered_df.empty:\n",
        "        return f\"í˜„ì¬ ì„ íƒí•˜ì‹  ì‹œê°„ëŒ€({time})ì—ëŠ” ì˜ì—…í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    filtered_df = filtered_df.reset_index(drop=True).head(k)\n",
        "\n",
        "\n",
        "    # í˜„ì§€ì¸ ë§›ì§‘ ì˜µì…˜\n",
        "\n",
        "    # í”„ë¡¬í”„íŠ¸ì— ë°˜ì˜í•˜ì—¬ í™œìš©\n",
        "    if local_choice == 'ì œì£¼ë„ë¯¼ ë§›ì§‘':\n",
        "        local_choice = 'ì œì£¼ë„ë¯¼(í˜„ì§€ì¸) ë§›ì§‘'\n",
        "    elif local_choice == 'ê´€ê´‘ê° ë§›ì§‘':\n",
        "        local_choice = 'í˜„ì§€ì¸ ë¹„ì¤‘ì´ ë‚®ì€ ê´€ê´‘ê° ë§›ì§‘'\n",
        "\n",
        "    # ì„ íƒëœ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì²˜ë¦¬\n",
        "    if filtered_df.empty:\n",
        "        return \"ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "\n",
        "    # ì°¸ê³ í•  ì •ë³´ì™€ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
        "    reference_info = \"\"\n",
        "    for idx, row in filtered_df.iterrows():\n",
        "        reference_info += f\"{row['text']}\\n\"\n",
        "\n",
        "    # ì‘ë‹µì„ ë°›ì•„ì˜¤ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
        "    prompt = f\"ì§ˆë¬¸: {question} íŠ¹íˆ {local_choice}ì„ ì„ í˜¸í•´\\nì°¸ê³ í•  ì •ë³´:\\n{reference_info}\\nì‘ë‹µ:\"\n",
        "\n",
        "    if print_prompt:\n",
        "        print('-----------------------------'*3)\n",
        "        print(prompt)\n",
        "        print('-----------------------------'*3)\n",
        "\n",
        "    # ì‘ë‹µ ìƒì„±\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(): # (disabled=not replicate_api):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            # response = generate_llama2_response(prompt)\n",
        "            response = generate_response_with_faiss(prompt, df, embeddings, model, embed_text, time, local_choice)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "\n",
        "            # ë§Œì•½ responseê°€ GenerateContentResponse ê°ì²´ë¼ë©´, ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "            if isinstance(response, str):\n",
        "                full_response = response\n",
        "            else:\n",
        "                full_response = response.text  # response ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ë¶€ë¶„ ì¶”ì¶œ\n",
        "\n",
        "            # for item in response:\n",
        "            #     full_response += item\n",
        "            #     placeholder.markdown(full_response)\n",
        "\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_AN8FUTZvsj",
        "outputId": "f4477323-c025-41d5-ef55-8739c40208d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tipI2mLqZ7V-",
        "outputId": "b074b1fa-406b-445a-82c0-0c8ce97ced70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.123.119.117\n",
            "your url is: https://red-terms-fail.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "akyiNI_taLel"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}