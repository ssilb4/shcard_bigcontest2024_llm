{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19cnqxZBxSaTQWv3RWggT2nyq9jn3rGxi",
      "authorship_tag": "ABX9TyOlAC16PHoBU+ac0DlwttV1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7mcWJh3Zrzo"
      },
      "outputs": [],
      "source": [
        "#원소스 출처: https://github.com/greenjhp/shcard_bigcontest2024_llm\n",
        "!apt install libomp-dev\n",
        "!pip install faiss-gpu\n",
        "!pip install streamlit\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#보안 비밀\n",
        "from google.colab import userdata\n",
        "import os\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "Fobu0VHEZuMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import faiss\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "# 경로 설정\n",
        "data_path = '/content/drive/MyDrive/shinhan_bigcontest/data'\n",
        "module_path = '/content/drive/MyDrive/shinhan_bigcontest/module'\n",
        "\n",
        "# Gemini 설정\n",
        "import google.generativeai as genai\n",
        "\n",
        "GOOGLE_API_KEY=os.environ['GOOGLE_API_KEY']\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Gemini 모델 선택\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "\n",
        "# CSV 파일 로드\n",
        "## 자체 전처리를 거친 데이터 파일 활용\n",
        "csv_file_path = data_path + \"/JEJU_MCT_DATA_modified.csv\"\n",
        "df = pd.read_csv(os.path.join(data_path, csv_file_path))\n",
        "\n",
        "# 최신연월 데이터만 가져옴\n",
        "df = df[df['기준연월'] == df['기준연월'].max()].reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "# Streamlit App UI\n",
        "\n",
        "st.set_page_config(page_title=\"🍊참신한 제주 맛집!\")\n",
        "\n",
        "# Replicate Credentials\n",
        "with st.sidebar:\n",
        "    st.title(\"🍊참신한! 제주 맛집\")\n",
        "\n",
        "    st.write(\"\")\n",
        "\n",
        "    st.subheader(\"언드레 가신디가?\")\n",
        "\n",
        "    # selectbox 레이블 공백 제거\n",
        "    st.markdown(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "        .stSelectbox label {  /* This targets the label element for selectbox */\n",
        "            display: none;  /* Hides the label element */\n",
        "        }\n",
        "        .stSelectbox div[role='combobox'] {\n",
        "            margin-top: -20px; /* Adjusts the margin if needed */\n",
        "        }\n",
        "        </style>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "    time = st.sidebar.selectbox(\"\", [\"아침\", \"점심\", \"오후\", \"저녁\", \"밤\"], key=\"time\")\n",
        "\n",
        "    st.write(\"\")\n",
        "\n",
        "    st.subheader(\"어드레가 맘에 드신디가?\")\n",
        "\n",
        "    # radio 레이블 공백 제거\n",
        "    st.markdown(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "        .stRadio > label {\n",
        "            display: none;\n",
        "        }\n",
        "        .stRadio > div {\n",
        "            margin-top: -20px;\n",
        "        }\n",
        "        </style>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True\n",
        "    )\n",
        "\n",
        "    local_choice = st.radio(\n",
        "        '',\n",
        "        ('제주도민 맛집', '관광객 맛집')\n",
        "    )\n",
        "\n",
        "    st.write(\"\")\n",
        "\n",
        "st.title(\"혼저 옵서예!👋\")\n",
        "st.subheader(\"군맛난 제주 밥집🧑‍🍳 추천해드릴게예\")\n",
        "\n",
        "st.write(\"\")\n",
        "\n",
        "st.write(\"#흑돼지 #갈치조림 #옥돔구이 #고사리해장국 #전복뚝배기 #한치물회 #빙떡 #오메기떡..🤤\")\n",
        "\n",
        "st.write(\"\")\n",
        "\n",
        "image_path = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRTHBMuNn2EZw3PzOHnLjDg_psyp-egZXcclWbiASta57PBiKwzpW5itBNms9VFU8UwEMQ&usqp=CAU\"\n",
        "image_html = f\"\"\"\n",
        "<div style=\"display: flex; justify-content: center;\">\n",
        "    <img src=\"{image_path}\" alt=\"centered image\" width=\"50%\">\n",
        "</div>\n",
        "\"\"\"\n",
        "st.markdown(image_html, unsafe_allow_html=True)\n",
        "\n",
        "st.write(\"\")\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"어드런 식당 찾으시쿠과?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"어드런 식당 찾으시쿠과?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "\n",
        "# RAG\n",
        "\n",
        "# 디바이스 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Hugging Face의 사전 학습된 임베딩 모델과 토크나이저 로드\n",
        "model_name = \"jhgan/ko-sroberta-multitask\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "embedding_model = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "print(f'Device is {device}.')\n",
        "\n",
        "\n",
        "# FAISS 인덱스 로드 함수\n",
        "def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index.index')):\n",
        "    \"\"\"\n",
        "    FAISS 인덱스를 파일에서 로드합니다.\n",
        "\n",
        "    Parameters:\n",
        "    index_path (str): 인덱스 파일 경로.\n",
        "\n",
        "    Returns:\n",
        "    faiss.Index: 로드된 FAISS 인덱스 객체.\n",
        "    \"\"\"\n",
        "    if os.path.exists(index_path):\n",
        "        # 인덱스 파일에서 로드\n",
        "        index = faiss.read_index(index_path)\n",
        "        print(f\"FAISS 인덱스가 {index_path}에서 로드되었습니다.\")\n",
        "        return index\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"{index_path} 파일이 존재하지 않습니다.\")\n",
        "# 텍스트 임베딩\n",
        "def embed_text(text):\n",
        "    # 토크나이저의 출력도 GPU로 이동\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        # 모델의 출력을 GPU에서 연산하고, 필요한 부분을 가져옴\n",
        "        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)\n",
        "    return embeddings.squeeze().cpu().numpy()  # 결과를 CPU로 이동하고 numpy 배열로 변환\n",
        "\n",
        "# 임베딩 로드\n",
        "embeddings = np.load(os.path.join(module_path, 'embeddings_array_file.npy'))\n",
        "\n",
        "def generate_response_with_faiss(question, df, embeddings, model, embed_text, time, local_choice, index_path=os.path.join(module_path, 'faiss_index.index'), max_count=10, k=3, print_prompt=True):\n",
        "    filtered_df = df\n",
        "\n",
        "    # FAISS 인덱스를 파일에서 로드\n",
        "    index = load_faiss_index(index_path)\n",
        "\n",
        "    # 검색 쿼리 임베딩 생성\n",
        "    query_embedding = embed_text(question).reshape(1, -1)\n",
        "\n",
        "    # 가장 유사한 텍스트 검색 (3배수)\n",
        "    distances, indices = index.search(query_embedding, k*3)\n",
        "\n",
        "    # FAISS로 검색된 상위 k개의 데이터프레임 추출\n",
        "    filtered_df = filtered_df.iloc[indices[0, :]].copy().reset_index(drop=True)\n",
        "\n",
        "\n",
        "    # 웹페이지의 사이드바에서 선택하는 영업시간, 현지인 맛집 조건 구현\n",
        "\n",
        "    # 영업시간 옵션\n",
        "    # 필터링 조건으로 활용\n",
        "\n",
        "    # 영업시간 조건을 만족하는 가게들만 필터링\n",
        "    if time == '아침':\n",
        "        filtered_df = filtered_df[filtered_df['영업시간'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(5, 12)))].reset_index(drop=True)\n",
        "    elif time == '점심':\n",
        "        filtered_df = filtered_df[filtered_df['영업시간'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(12, 14)))].reset_index(drop=True)\n",
        "    elif time == '오후':\n",
        "        filtered_df = filtered_df[filtered_df['영업시간'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(14, 18)))].reset_index(drop=True)\n",
        "    elif time == '저녁':\n",
        "        filtered_df = filtered_df[filtered_df['영업시간'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(18, 23)))].reset_index(drop=True)\n",
        "    elif time == '밤':\n",
        "        filtered_df = filtered_df[filtered_df['영업시간'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in [23, 24, 1, 2, 3, 4]))].reset_index(drop=True)\n",
        "\n",
        "    # 필터링 후 가게가 없으면 메시지를 반환\n",
        "    if filtered_df.empty:\n",
        "        return f\"현재 선택하신 시간대({time})에는 영업하는 가게가 없습니다.\"\n",
        "\n",
        "    filtered_df = filtered_df.reset_index(drop=True).head(k)\n",
        "\n",
        "\n",
        "    # 현지인 맛집 옵션\n",
        "\n",
        "    # 프롬프트에 반영하여 활용\n",
        "    if local_choice == '제주도민 맛집':\n",
        "        local_choice = '제주도민(현지인) 맛집'\n",
        "    elif local_choice == '관광객 맛집':\n",
        "        local_choice = '현지인 비중이 낮은 관광객 맛집'\n",
        "\n",
        "    # 선택된 결과가 없으면 처리\n",
        "    if filtered_df.empty:\n",
        "        return \"질문과 일치하는 가게가 없습니다.\"\n",
        "\n",
        "\n",
        "    # 참고할 정보와 프롬프트 구성\n",
        "    reference_info = \"\"\n",
        "    for idx, row in filtered_df.iterrows():\n",
        "        reference_info += f\"{row['text']}\\n\"\n",
        "\n",
        "    # 응답을 받아오기 위한 프롬프트 생성\n",
        "    prompt = f\"질문: {question} 특히 {local_choice}을 선호해\\n참고할 정보:\\n{reference_info}\\n응답:\"\n",
        "\n",
        "    if print_prompt:\n",
        "        print('-----------------------------'*3)\n",
        "        print(prompt)\n",
        "        print('-----------------------------'*3)\n",
        "\n",
        "    # 응답 생성\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(): # (disabled=not replicate_api):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            # response = generate_llama2_response(prompt)\n",
        "            response = generate_response_with_faiss(prompt, df, embeddings, model, embed_text, time, local_choice)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "\n",
        "            # 만약 response가 GenerateContentResponse 객체라면, 문자열로 변환하여 사용합니다.\n",
        "            if isinstance(response, str):\n",
        "                full_response = response\n",
        "            else:\n",
        "                full_response = response.text  # response 객체에서 텍스트 부분 추출\n",
        "\n",
        "            # for item in response:\n",
        "            #     full_response += item\n",
        "            #     placeholder.markdown(full_response)\n",
        "\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_AN8FUTZvsj",
        "outputId": "f4477323-c025-41d5-ef55-8739c40208d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tipI2mLqZ7V-",
        "outputId": "b074b1fa-406b-445a-82c0-0c8ce97ced70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.123.119.117\n",
            "your url is: https://red-terms-fail.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "akyiNI_taLel"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}